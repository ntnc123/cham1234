#Th∆∞ vi·ªán
#pip install python-docx
import os
import nltk
from gensim.models import Word2Vec
from sklearn.cluster import KMeans
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np
import pickle

nltk.download("punkt")

# ===== B∆∞·ªõc 1: T√°ch c√¢u v√† token h√≥a ti·∫øng Vi·ªát =====
def preprocess_text(text):
    text = text.strip()
    # T√°ch c√¢u: d√πng NLTK v·ªõi h·ªó tr·ª£ ti·∫øng Vi·ªát (ƒë∆°n gi·∫£n)
    sentences = nltk.sent_tokenize(text, language='english')  # v·∫´n hi·ªáu qu·∫£ v·ªõi ti·∫øng Vi·ªát ƒë∆°n gi·∫£n
    return sentences

def tokenize_sentences(sentences):
    return [nltk.word_tokenize(sent.lower()) for sent in sentences]

# ===== B∆∞·ªõc 2: Train ho·∫∑c load m√¥ h√¨nh Word2Vec =====
def train_or_load_word2vec(all_tokenized_sentences, model_path="word2vec_vi.model"):
    if os.path.exists(model_path):
        print("üîÅ Loading Word2Vec model t·ª´ file...")
        model = Word2Vec.load(model_path)
    else:
        print("üß† Training Word2Vec model...")
        model = Word2Vec(sentences=all_tokenized_sentences, vector_size=128, window=5, min_count=1)
        model.save(model_path)
    return model

def sentence_vector(model, tokenized_sentence):
    vectors = [model.wv[word] for word in tokenized_sentence if word in model.wv]
    return np.mean(vectors, axis=0) if vectors else np.zeros(model.vector_size)

# ===== B∆∞·ªõc 3: Ph√¢n c·ª•m v√† ch·ªçn c√¢u ƒë·∫°i di·ªán =====
def cluster_sentences(sent_vectors, n_clusters):
    kmeans = KMeans(n_clusters=n_clusters, random_state=42)
    kmeans.fit(sent_vectors)
    return kmeans.labels_, kmeans.cluster_centers_

def find_representative_sentences(sent_vectors, labels, centers):
    representatives = []
    for i in range(len(centers)):
        indices = [j for j, label in enumerate(labels) if label == i]
        if indices:
            sims = cosine_similarity([centers[i]], [sent_vectors[j] for j in indices])
            best_index = indices[np.argmax(sims)]
            representatives.append(best_index)
    return sorted(representatives)

def generate_summary(sentences, rep_indices):
    return ' '.join([sentences[i] for i in rep_indices])

# ===== B∆∞·ªõc 4: T√≥m t·∫Øt m·ªôt vƒÉn b·∫£n =====
def summarize_text(text, model):
    sentences = preprocess_text(text)
    tokenized = tokenize_sentences(sentences)
    sent_vectors = np.array([sentence_vector(model, sent) for sent in tokenized])
    n_clusters = min(3, len(sentences))  # s·ªë c·ª•m t·ªëi ƒëa l√† s·ªë c√¢u
    labels, centers = cluster_sentences(sent_vectors, n_clusters)
    rep_indices = find_representative_sentences(sent_vectors, labels, centers)
    return generate_summary(sentences, rep_indices)

#documents = [
 #   """√ît√¥ t·∫£i b·ªã c·∫•m v√†o 5 tuy·∫øn ƒë∆∞·ªùng ·ªü huy·ªán B√¨nh Ch√°nh trong 6 ng√†y ƒë·ªÉ ƒë·∫£m an to√†n khi t·ªï ch·ª©c ƒë·∫°i l·ªÖ ph·∫≠t ƒë·∫£n Vesak...""",
#    """Theo S·ªü X√¢y d·ª±ng TP HCM, th·ªùi gian c·∫•m xe t·∫£i √°p d·ª•ng t·ª´ ng√†y 3 ƒë·∫øn 8/5...""",
#]
# T·∫°o danh s√°ch vƒÉn b·∫£n ƒë·ªÉ t√≥m t·∫Øt
from docx import Document

def read_docx(file_path):
    doc = Document(file_path)
    full_text = '\n'.join([para.text for para in doc.paragraphs if para.text.strip()])
    return full_text

file_path = "taytien.docx"
text = read_docx(file_path)  # ƒê·ªçc to√†n b·ªô file Word
sentences = preprocess_text(text)
tokenized = tokenize_sentences(sentences)


# Train ho·∫∑c load Word2Vec
model = train_or_load_word2vec(tokenized)


# T√≥m t·∫Øt
summary = summarize_text(text, model)

print("\n=== T√ìM T·∫ÆT ===")
print(summary)
